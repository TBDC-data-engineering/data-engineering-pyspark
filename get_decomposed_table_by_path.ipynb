{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695f5dfa",
   "metadata": {},
   "source": [
    "Events Decomposition and Data Extraction\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "120ba33e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Algorithm: get_decomposed_table_by_path\n",
    "Purpose:\n",
    "Given a nested path (e.g., a.b.c), this method extracts and flattens the relevant part of a DataFrame according to the schema and relationships, handling both struct and array types.\n",
    "\n",
    "Steps\n",
    "Initialize:\n",
    "    Start with the full DataFrame with a column where data string is already converted to struct using from_json (whole_df).\n",
    "    Iterate over the composition elements and for a given composition element, split it into path segments (e.g., \"a.b.c\") into segments ([\"a\", \"b\", \"c\"]).\n",
    "     Prepare an empty list for the current relation path.\n",
    "\n",
    "Iterate Over Path Segments:\n",
    "   For each segment in the path: \n",
    "       Add the segment to the current relation path.\n",
    "       Build the field name for the current level (e.g., \"a\", \"a.b\", \"a.b.c\").\n",
    "Get Sub-Schema:\n",
    "         For the current field, extract the relevant sub-schema from the overall Spark schema.\n",
    "         Check Field Type and Process:\n",
    "            If the field is a StructType:\n",
    "              Select all fields under this struct, flattening them into columns.\n",
    "              Apply any relationship logic if needed.\n",
    "           If the field is an ArrayType:\n",
    "              Select the array column and any technical fos system columns.\n",
    "              Use explode_outer to flatten the array into rows.\n",
    "              Select all fields from the exploded struct as columns.\n",
    "              Apply any relationship logic if needed.\n",
    "           If the field is a primitive type:\n",
    "             Select it directly.\n",
    "Rename Columns:\n",
    "         After flattening, rename columns as needed to remove path prefixes and match the target schema.\n",
    "Return:\n",
    "    Return the resulting DataFrame, which contains the decomposed and flattened data for the requested path.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a63f2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parquet file to: /data/events/data/root\n",
      "Writing parquet file to: /data/events/data/data\n",
      "Writing parquet file to: /data/events/data/data.assignmentDetail.standardAssignmentDetail\n",
      "Writing parquet file to: /data/events/data/data.jobDetails\n",
      "Writing parquet file to: /data/events/data/jobDetailUSA\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "def process_nested_fields(df, field_name, fields, extra_directory=None):    \n",
    "    output_table_fields = []\n",
    "    for field in fields:        \n",
    "        if not isinstance(field.dataType, (ArrayType, StructType)):\n",
    "            path_name = \".\".join([field_name, field.name]).strip(\".\")\n",
    "            output_table_fields.append(path_name)\n",
    "    \n",
    "    if output_table_fields:\n",
    "        if extra_directory:\n",
    "            parquet_path = extra_directory\n",
    "        else:\n",
    "            parquet_path = field_name if field_name else \"root\"\n",
    "\n",
    "        print(f\"Writing parquet file to: /data/events/data/{parquet_path}\")\n",
    "        df.select(output_table_fields).write.mode(\"overwrite\").parquet(f\"/data/events/data/{parquet_path}\")\n",
    "    \n",
    "\n",
    "    nested_struct_fields = [f for f in fields if isinstance(f.dataType, StructType)]    \n",
    "    for struct_field in nested_struct_fields:        \n",
    "        path_name = \".\".join([field_name, struct_field.name]).strip(\".\")       \n",
    "        process_nested_fields(df, path_name, struct_field.dataType.fields)\n",
    "\n",
    "\n",
    "    nested_array_fields = [f.name for f in fields if isinstance(f.dataType, ArrayType)]\n",
    "    for array_field in nested_array_fields:\n",
    "        path_name = \".\".join([field_name, array_field]).strip(\".\")                \n",
    "        nested_df = df.select(explode(col(path_name)).alias(field_name)).select(field_name + \".*\")                \n",
    "        process_nested_fields(nested_df,\"\", nested_df.schema.fields, path_name)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataJsonProcessing\").getOrCreate()\n",
    "\n",
    "data_path = \"/data/events/sap.sf.workforce.assignment.updated.v1_20250708130214.jsonl\"\n",
    "\n",
    "events_dataframe = spark.read.json(data_path)\n",
    "\n",
    "process_nested_fields(events_dataframe, \"\", events_dataframe.schema.fields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
