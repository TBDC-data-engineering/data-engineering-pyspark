{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge in Python: Which One Has the Nicest Syntax and Is the Fastest?\n",
    "# https://databrickster.medium.com/merge-in-python-which-one-has-the-nicest-syntax-and-is-the-fastest-845799729c23\n",
    "https://databrickster.medium.com/merge-in-python-which-one-has-the-nicest-syntax-and-is-the-fastest-845799729c23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee793283",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 0.  TEST RUNTIME SETTINGS\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.autoBroadcastJoinThreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# force shuffle joins\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  TEST RUNTIME SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)   # force shuffle joins\n",
    "spark.conf.set(\"spark.databricks.optimizer.dynamicFilePruning\", \"true\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  BUILD WIDE, COMPLEX, SKEWED DATAFRAMES\n",
    "#      – many columns\n",
    "#      – nested/array column\n",
    "#      – heavy computed column\n",
    "#      – 90 % skew on tenant_id\n",
    "# ------------------------------------------------------------------\n",
    "NUM_SOURCE_ROWS  = 100_000_000\n",
    "NUM_TARGET_ROWS  = 200_000_000           # 50 M overlap + 150 M new\n",
    "SKEW_FRACTION    = 0.90                  # 90 % rows get same tenant_id\n",
    "\n",
    "def build_df(start: int, end: int):\n",
    "    df = spark.range(start, end).repartition(512)            # plenty of partitions\n",
    "    df = (df\n",
    "          .withColumn(\"date\", F.expr(\"current_date() - cast(rand()*100 as int)\"))\n",
    "          .withColumn(\"status\", F.when(F.rand() > 0.5, \"active\").otherwise(\"inactive\"))\n",
    "          .withColumn(\"tenant_id\",\n",
    "                      F.when(F.rand() < SKEW_FRACTION, F.lit(\"TenantA\"))\n",
    "                       .otherwise(F.concat(F.lit(\"Tenant\"), F.expr(\"cast(rand()*1000 as int)\"))))\n",
    "          # --- width boosters ---\n",
    "          .withColumn(\"value1\",  F.rand()*1000)\n",
    "          .withColumn(\"value2\",  F.col(\"id\")*5 + F.lit(123))\n",
    "          .withColumn(\"status_flag\", F.when(F.col(\"status\") == \"active\", 1).otherwise(0))\n",
    "          .withColumn(\"id_str\",  F.concat(F.lit(\"ID-\"), F.col(\"id\")))\n",
    "          .withColumn(\"address\",\n",
    "                      F.struct(F.lit(\"123 Main St\").alias(\"street\"),\n",
    "                               F.lit(\"Metropolis\").alias(\"city\")))\n",
    "          .withColumn(\"random_values\", F.array(F.rand(), F.rand(), F.rand()))\n",
    "          .withColumn(\"complex_calc\",\n",
    "                      F.pow(F.col(\"id\").cast(\"double\"), 2) * F.log(F.col(\"id\")+1))\n",
    "          .withColumn(\"last_updated\", F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "source_df  = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "target_df  = build_df(50_000_000, 50_000_000 + NUM_TARGET_ROWS)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  (RE)CREATE TARGET TABLES\n",
    "# ------------------------------------------------------------------\n",
    "for tbl in (\"target1\", \"target2\", \"target3\"):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
    "    target_df.write.mode(\"overwrite\").saveAsTable(tbl)\n",
    "    spark.sql(f\"OPTIMIZE {tbl}\") \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  DEFINE A HELPER TO TIME MERGES CLEANLY\n",
    "# ------------------------------------------------------------------\n",
    "def time_it(label, fn):\n",
    "    start = time.perf_counter()\n",
    "    fn()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{label:<12s}  {elapsed:,.1f}  seconds\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  MERGE PATTERN 1  – DeltaTable API\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_1():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)               # fresh source (avoid cache)\n",
    "    DeltaTable.forName(spark, \"target1\") \\\n",
    "      .alias(\"t\") \\\n",
    "      .merge(src.alias(\"s\"), \"t.id = s.id\") \\\n",
    "      .whenMatchedUpdate(set = {\n",
    "          # heavy conditional update to stress CPU\n",
    "          \"status\": \"CASE WHEN t.status = 'inactive' AND s.status = 'active' \"\n",
    "                    \"THEN 'reactivated' ELSE t.status END\",\n",
    "          \"value1\": \"t.value1 + s.value1\",\n",
    "          \"status_flag\": \"CASE WHEN s.status = 'active' THEN 1 ELSE 0 END\",\n",
    "          \"last_updated\": \"current_timestamp()\"\n",
    "      }) \\\n",
    "      .whenNotMatchedInsertAll() \\\n",
    "      .execute()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  MERGE PATTERN 2  – DataFrameWriter.mergeInto\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_2():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "    (src.alias(\"s\")\n",
    "        .mergeInto(\"target2\", F.expr(\"target2.id = s.id\"))\n",
    "        .whenMatched()\n",
    "            .update({\n",
    "                \"status\":      F.expr(\"CASE WHEN target2.status = 'inactive' \"\n",
    "                                      \"AND s.status = 'active' \"\n",
    "                                      \"THEN 'reactivated' ELSE target2.status END\"),\n",
    "                \"value1\":      F.expr(\"target2.value1 + s.value1\"),\n",
    "                \"status_flag\": F.when(src.status == \"active\", 1).otherwise(0),\n",
    "                \"last_updated\": F.current_timestamp()\n",
    "            })\n",
    "        .whenNotMatched()\n",
    "            .insertAll()\n",
    "        .merge())\n",
    "    \n",
    "# ------------------------------------------------------------------\n",
    "# 6.  MERGE PATTERN 3  – SQL in PySpark\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_3():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO target3 t\n",
    "        USING {src_df} s\n",
    "        ON   t.id = s.id\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            status       = CASE WHEN t.status = 'inactive' AND s.status = 'active'\n",
    "                                THEN 'reactivated' ELSE t.status END,\n",
    "            value1       = t.value1 + s.value1,\n",
    "            status_flag  = CASE WHEN s.status = 'active' THEN 1 ELSE 0 END,\n",
    "            last_updated = current_timestamp()\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "    \"\"\", src_df=src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
