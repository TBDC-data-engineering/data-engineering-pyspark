{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeacd27",
   "metadata": {},
   "source": [
    "## Schema Manipulation and Data Treatment and Handling missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a782c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import lower \n",
    "schema = StructType([    \n",
    "        StructField(\"user_id\", IntegerType()),\n",
    "        StructField(\"device_id\", IntegerType()),    \n",
    "        StructField(\"referrer\", StringType()),\n",
    "        StructField(\"host\", StringType()),\n",
    "        StructField(\"url\", StringType()),\n",
    "        StructField(\"event_time\", StringType())])\n",
    "\n",
    "event_data_frame = spark.read.csv(\"file:///data/db1/events.csv\", sep=',', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25330a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404814\n",
      "Time taken to count rows: 0.06062579154968262 seconds\n",
      "404814\n",
      "Time taken to count rows: 0.14413046836853027 seconds\n",
      "True\n",
      "404814\n",
      "Time taken to count rows: 0.032175302505493164 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "print(event_data_frame.count())\n",
    "print(\"Time taken to count rows: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "event_data_frame.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "print(event_data_frame.count())\n",
    "print(\"Time taken to count rows: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "print(event_data_frame.is_cached)\n",
    "\n",
    "event_data_frame.unpersist()\n",
    "\n",
    "start_time = time.time()\n",
    "print(event_data_frame.count())\n",
    "print(\"Time taken to count rows: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf367f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+--------------------+----------+--------------------+----------+------------+-------+-----------+\n",
      "|    user_id| device_id|referrer|                host|       url|          event_time| device_id|browser_type|os_type|device_type|\n",
      "+-----------+----------+--------+--------------------+----------+--------------------+----------+------------+-------+-----------+\n",
      "| 1037710827| 532630305|    NULL| www.zachwilson.tech|         /|2021-03-08 17:27:...| 532630305|       Other|  Other|      Other|\n",
      "|  925588856| 532630305|    NULL|    www.eczachly.com|         /|2021-05-10 11:26:...| 532630305|       Other|  Other|      Other|\n",
      "|-1180485268| 532630305|    NULL|admin.zachwilson....|         /|2021-02-17 16:19:...| 532630305|       Other|  Other|      Other|\n",
      "|-1044833855| 532630305|    NULL| www.zachwilson.tech|         /|2021-09-24 15:53:...| 532630305|       Other|  Other|      Other|\n",
      "|  747494706| 532630305|    NULL| www.zachwilson.tech|         /|2021-09-26 16:03:...| 532630305|       Other|  Other|      Other|\n",
      "|  747494706| 532630305|    NULL|admin.zachwilson....|         /|2021-02-21 16:08:...| 532630305|       Other|  Other|      Other|\n",
      "| -824540328| 532630305|    NULL|admin.zachwilson....|         /|2021-09-28 17:23:...| 532630305|       Other|  Other|      Other|\n",
      "| -824540328| 532630305|    NULL|    www.eczachly.com|         /|2021-09-29 01:22:...| 532630305|       Other|  Other|      Other|\n",
      "| 1833036683| 532630305|    NULL|admin.zachwilson....|         /|2021-01-24 03:15:...| 532630305|       Other|  Other|      Other|\n",
      "|-2134824313| 532630305|    NULL|    www.eczachly.com|         /|2021-01-25 00:03:...| 532630305|       Other|  Other|      Other|\n",
      "|-1809929467|-906264142|    NULL|admin.zachwilson....|/.git/HEAD|2021-02-22 01:36:...|-906264142|        curl|  Other|      Other|\n",
      "| 2002285749|-906264142|    NULL|    www.eczachly.com|         /|2021-02-22 02:25:...|-906264142|        curl|  Other|      Other|\n",
      "|-1562965412| 532630305|    NULL| www.zachwilson.tech|         /|2021-01-30 20:46:...| 532630305|       Other|  Other|      Other|\n",
      "|-1099860451| 532630305|    NULL|    www.eczachly.com|         /|2021-02-04 23:49:...| 532630305|       Other|  Other|      Other|\n",
      "| 1246896869|-906264142|    NULL| www.zachwilson.tech|         /|2021-02-22 02:50:...|-906264142|        curl|  Other|      Other|\n",
      "| -629331502|-906264142|    NULL|admin.zachwilson....|/.git/HEAD|2021-02-22 23:51:...|-906264142|        curl|  Other|      Other|\n",
      "|-1913422462|-906264142|    NULL|    www.eczachly.com|         /|2021-02-23 00:17:...|-906264142|        curl|  Other|      Other|\n",
      "|   50429624| 532630305|    NULL|    www.eczachly.com|         /|2022-12-28 01:38:...| 532630305|       Other|  Other|      Other|\n",
      "|  222389292| 532630305|    NULL| www.zachwilson.tech|         /|2022-12-28 05:23:...| 532630305|       Other|  Other|      Other|\n",
      "| -779924777| 532630305|    NULL| www.zachwilson.tech|         /|2022-12-28 16:45:...| 532630305|       Other|  Other|      Other|\n",
      "+-----------+----------+--------+--------------------+----------+--------------------+----------+------------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/13 18:46:23 WARN S3FileIO: Unclosed S3FileIO instance created by:\n",
      "\torg.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)\n",
      "\torg.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)\n",
      "\torg.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)\n",
      "\torg.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)\n",
      "\torg.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)\n",
      "\torg.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)\n",
      "\torg.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n",
      "\torg.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n",
      "\tscala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)\n",
      "\tscala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tscala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tscala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tscala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
      "\tscala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
      "\tscala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tscala.collection.immutable.List.foreach(List.scala:431)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\torg.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n",
      "\torg.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n",
      "\torg.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)\n",
      "\torg.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)\n",
      "\torg.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)\n",
      "\torg.apache.spark.sql.Dataset.persist(Dataset.scala:3775)\n",
      "\torg.apache.spark.sql.Dataset.cache(Dataset.scala:3785)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tjava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tjava.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tpy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tpy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tpy4j.Gateway.invoke(Gateway.java:282)\n",
      "\tpy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tpy4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tpy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tpy4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "#Broadk Cast join example\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "event = spark.read.csv(\"file:///data/db1/events.csv\", sep=',', header=True, schema=schema)\n",
    "devices = spark.read.csv(\"file:///data/db1/devices.csv\", sep=',', header=True)\n",
    "\n",
    "even_devices = event.join(broadcast(devices), event.device_id == devices.device_id, \"inner\") \n",
    "\n",
    "even_devices.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adbfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [user_id#2151,device_id#2152,referrer#2153,host#2154,url#2155,event_time#2156] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/db1/events.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:int,device_id:int,referrer:string,host:string,url:string,event_time:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_data_frame.explain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f04a0606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySparkShell\n",
      "32977\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.app.name\"))\n",
    "\n",
    "print(spark.conf.get(\"spark.driver.port\"))\n",
    "\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
